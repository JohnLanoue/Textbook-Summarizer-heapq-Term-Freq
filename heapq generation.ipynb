{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "In this project we are building a spark notes generator.  In the event that I find myself a little crunched for time when reading a book and need necessary details rapidly, this program will *theoretically* take a few words off of my plate.  \n",
    "\n",
    "This project will showcase my skill with: webscraping, regular expressions, and scoring.  Although web scraping was certianly not a part of this course, the ends are highly necessary to have an option to read in data from most places, if it is acceptable to scape the page.  Regular expression were talked about extensivly in this course and is used to pick up speach patterns.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "Using the most common webscraping format, we import the data based off of a url and pick up the paragraphs.  I have tested this for limited sites, and if crucial information is in headdings or other htypes of tags, it will get missed.  This did not have to get built into a function, but I would like to keep this handy for reusablility.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "#PRE: beautiful soup and nltk are imported\n",
    "    scraped = urllib.request.urlopen(url).read()\n",
    "    parsed =  bs.BeautifulSoup(scraped, 'lxml')\n",
    "    paragraphs = parsed.find_all('p')\n",
    "    text = \"\"\n",
    "    for p in paragraphs:\n",
    "        text += p.text\n",
    "    #tokens = nltk.sent_tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data('https://www.nltk.org/book/ch01.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Much like any Data science project, just because we have the data, does not mean that it does not require any further refinement.  With the scraped data we hope to reduce the text to readable messages, tokenize the data, and rule out stopwords.\n",
    "## Removing texts \n",
    "Below we are replcing numbers, duplicate whitespace, and non-alpha's then turns them into whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = re.sub(r'\\[[0-9]*\\]', ' ', data)\n",
    "data = re.sub(r'\\s+', ' ', data)\n",
    "data = re.sub('[^a-zA-Z]', ' ', data)\n",
    "data = re.sub(r'\\s+', ' ', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize sentance\n",
    "Below we tokenize the individual sentances into their own object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Stopwords \n",
    "Instead of brainstorming my own set of stopwords, like I have in the past, I am simply poaching the nltk coprus collection.  This is a list of words that is far more elaborate than what I could come up with and is a hundred times easier to develop.  This list consists of 179 words that are all too common in speech to see recognize a signigificant use.  \n",
    "\n",
    "The reason that eliminating stopwords is so important in this project is that our stratagy is dependant on the word frequency in each sentance, and using common words would cause an unnecessary noise that the model would not be able to see through.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency\n",
    "Below we fufil our strategy of getting the word frequency of each word in the sentances.  We include the word frequncy because theoretically, if a word is used more frequently, the sentance it pertains to is more important.  Thus we want to iterate through the article to find the most freqently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency(formatted_article):\n",
    "    word_frequency = {}\n",
    "    for word in nltk.word_tokenize(formatted_article):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequency.keys():\n",
    "                word_frequency[word] = 1\n",
    "            else: \n",
    "                word_frequency[word] += 1\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequncy = get_word_frequency(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentance Score\n",
    "Similar to above, we need to iterate through the entire text.  Utilizing the frequncy from above, we sum them into the score below and  get the relevance score of all of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentance_score(tokenize_sentance, freq):\n",
    "    score = {}\n",
    "    for sent in tokenize_sentence:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in freq.keys():\n",
    "                if len(sent.split(' ')) < 50:\n",
    "                    if sent not in score.keys():\n",
    "                        score[sent] = freq[word]\n",
    "                    else:\n",
    "                        score[sent] += freq[word]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentance_score = get_sentance_score(tokens, word_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output the sparknotes\n",
    "Finally, we have built the scores for getting the most important information.  Now using the heapq package we can output them in a priority queue with the most important data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next example shows us that the number of distinct words is just 6% of the total number of words, or equivalently that each word is used 16 times on average (remember if you're using Python 2, to start with from __future__ import division).Next, let's focus on particular words. These 50 words account for nearly half the book!Figure 3.2: Cumulative Frequency Plot for 50 Most Frequently Words in Moby Dick: these account for nearly half of the tokens.If the frequent words don't help us, how about the words that occur once only, the so-called hapaxes? One might wonder how frequent the different lengths of word are (e.g., how many words of length four appear in the text, are there more words of length five than length four, etc). We can count how often a word occurs in a text, and compute what percentage of the text is taken up by a specific word:NoteYour Turn: How many times does the word lol appear in text5? Does it make a difference to your results if you change the variable names, e.g., using [word for word in vocab if ...]?Let's return to our task of finding words that characterize a text. A characteristic of collocations is that they are resistant to substitution with words that have similar senses; for example, maroon wine sounds definitely odd.To get a handle on collocations, we start off by extracting from a text a list of word pairs, also known as bigrams. In the next chapter you will learn how to access a broader range of text, including text in languages other than English.A concordance permits us to see words in context. Use a combination of for and if statements to loop over the words of the movie script for Monty Python and the Holy Grail (text6) and print all the uppercase words, one per line.◑ Write expressions for finding all words in text6 that meet the conditions listed below. Since neither frequent nor infrequent words help, we need to try something else.Next, let's look at the long words of a text; perhaps these will be more characteristic and informative. When we append() to a list, the list itself is updated as a result of the operation.As we have seen, a text in Python is a list of words, represented using a combination of brackets and quotes. If you're not sure how to do this task, it would be a good idea to review the previous section before continuing further.How can we automatically identify the words of a text that are most informative about the topic and genre of the text? If you get an error message NameError: name 'FreqDist' is not defined, you need to start your work with from nltk.book import *Do any words produced in the last example help us grasp the topic or genre of this text? In other words, we automatically disambiguate words using context, exploiting the simple fact that nearby words have closely related meanings. A word type is the form or spelling of the word independently of its specific occurrences in a text — that is, the word considered as a unique item of vocabulary. We can do this as follows:From this we see that the most frequent word length is 3, and that words of length 3 account for roughly 50,000 (or 20%) of the words making up the book. Will this be the case for other texts?◑ What is the difference between the following two tests: w.isupper() and not w.islower()?◑ Write the slice expression that extracts the last two words of text2.◑ Find all the four-letter words in the Chat Corpus (text5). Austen uses this word quite differently from Melville; for her, monstrous has positive connotations, and sometimes functions as an intensifier like the word very.The term common_contexts allows us to examine just the contexts that are shared by two or more words, such as monstrous and very. You could look at text4, the Inaugural Address Corpus, to see examples of English going back to 1789, and search for words like nation, terror, god to see how these words have been used differently over time. The result should be in the form of a list of words: ['word1', 'word2', ...].◑ Define sent to be the list of words ['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']. Adding two lists creates a new list with everything from the first list, followed by everything from the second list:NoteThis special use of the addition operation is called concatenation; it combines the lists together into a single list. Please see http://nltk.org/ for installation instructions.NoteYou can also plot the frequency of word usage through time using https://books.google.com/ngramsNow, just for fun, let's try generating some random text in the various styles we have just seen. Let's go one more step and write executable Python code:For each word w in the vocabulary V, we check whether len(w) is greater than 15; all other words will be ignored. However, we can also determine the location of a word in the text: how many words from the beginning it appears. Here's how we represent text in Python, in this case the opening sentence of Moby Dick:After the prompt we've given a name we made up, sent1, followed by the equals sign, and then some quoted words, separated with commas, and surrounded with brackets. Let's use a FreqDist to find the 50 most frequent words of Moby Dick:When we first invoke FreqDist, we pass the name of the text as an argument . Test your understanding by modifying the examples, and trying the exercises at the end of the chapter.Let's begin by finding out the length of a text from start to finish, in terms of the words and punctuation symbols that appear. Here are some examples of variables and assignments:Remember that capitalized words appear before lowercase words in sorted lists.NoteNotice in the previous example that we split the definition of my_sent over two lines. The full set of these relational operators is shown in 4.1.Table 4.1: Numerical Comparison OperatorsWe can use these to select different words from a sentence of news text. In this section we will see how to use the computer to count the words in a text in a variety of useful ways. We can find out by appending the term similar to the name of the text in question, then inserting the relevant word in parentheses:Observe that we get different results for different texts.\n"
     ]
    }
   ],
   "source": [
    "import heapq \n",
    "sentence_summary = heapq.nlargest(30, sentence_score, key = sentence_score.get)\n",
    "summary = ' '.join(sentence_summary)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
